{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are going to complete our project in four steps.**\n",
    "\n",
    "    1.Creating a model using ML and NLP\n",
    "    2.Creating a web app using flask and connecting it with model\n",
    "    3.Commit project to Github\n",
    "    4.Deploy our model using Heroku\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  ## Preprocessing our text\n",
    "from nltk.corpus import stopwords ## removing all the stop words\n",
    "from nltk.stem.porter import PorterStemmer ## stemming of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK:** Natural Language Processing Toolkit is a python library that is used for performing all the NLP tasks like stemming, lemmatizing or removing stopwords, etc.\n",
    "\n",
    "**Porter Stemmer:** It is a type of stemmer that is used for stemming. stemming is basically a technique of converting a word to its root word.\n",
    "\n",
    "Ex: learning → learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset which we are going to use is an open-source dataset available on Kaggle.\n",
    "\n",
    "Dataset: [Spam-Ham Data](https://www.kaggle.com/venky73/spam-mails-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the dataset**\n",
    "\n",
    "The dataset contains three columns. The size of the dataset is around 5.65mb. It has around 5000 rows in total.\n",
    "Columns\n",
    "\n",
    "**Label:** ham, spam\n",
    "    \n",
    "**Text:** a collection of text or emails\n",
    "    \n",
    "Label_num: 0 for ham and 1 for spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "    Our Task is to create a machine learning model that can accurately predict whether an email is a spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load our dataset into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spamhamdata.csv\",sep=\"\\t\",names=[\"label\",\"message\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n",
      "2\n",
      "11144\n",
      "label      0\n",
      "message    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "label      5572 non-null object\n",
      "message    5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n",
      "None\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "     label                                            message\n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
      "5568   ham               Will ü b going to esplanade fr home?\n",
      "5569   ham  Pity, * was in mood for that. So...any other s...\n",
      "5570   ham  The guy did some bitching but I acted like i'd...\n",
      "5571   ham                         Rofl. Its true to its name\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)  ### Return the shape of data \n",
    "print(df.ndim)   ### Return the n dimensions of data\n",
    "print(df.size)   ### Return the size of data \n",
    "print(df.isna().sum())  ### Returns the sum fo all na values\n",
    "print(df.info())  ### Give concise summary of a DataFrame\n",
    "print(df.head())  ## top 5 rows of the dataframe\n",
    "print(df.tail()) ## bottom 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code block you will see that we don’t have any null values in our dataset. Also, one thing to notice is that only one column of our has numerical values so we can only visualize that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"label\"].value_counts().plot(kind=\"bar\",figsize=(12,6))\n",
    "plt.xticks(np.arange(2), ('Non spam', 'spam'),rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"message\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any NLP problem, the most important step is to clean the text. cleaning text means removing all the punctuation, removing stopwords, performing stemming, lemmatization, and converting the text into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    # specific\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    # general\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = text.replace('\\\\r', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    text = text.replace('\\\\\"', ' ')\n",
    "    #text = text.replace('!!!\"', ' ')\n",
    "    text = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z0-9']+)|(\\w+:\\/\\/\\S+)\", ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    text = ' '.join(e for e in text.split() if e not in stopwords.words(\"english\"))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"message\"] = df[\"message\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go jurong point crazy available bugis n great world la e buffet cine got amore wat'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"message\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think go usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  go jurong point crazy available bugis n great ...\n",
       "1   ham                              ok lar joke wif u oni\n",
       "2  spam  free entry 2 wkly comp win fa cup final tkts 2...\n",
       "3   ham                u dun say early hor u c already say\n",
       "4   ham                nah think go usf live around though"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Lemmatization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "w = WordNetLemmatizer()\n",
    "df.message=df.message.apply(lambda x:' '.join([w.lemmatize(word,'v') for word in x.split()])) # v stands for verb\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line 1: we are importing re library, which is used to perform regex in python.<br>\n",
    "Line 2: Define an empty corpus list, that can be used to store all the text after cleaning.<br>\n",
    "Line 3: initializing the var length with the length of the data frame.<br>\n",
    "Line 4: running a loop from 0 to the length of our data frame.<br>\n",
    "Line 5: Removing all characters except the lower alphabet, bigger alphabets, and digits.<br>\n",
    "Line 6: Converting the text to lower.<br>\n",
    "Line7: Splitting the text by spaces.<br>\n",
    "Line 8: creating an object of porter stemmer.<br>\n",
    "Line9: Initializing all the stopword in English dictionary to var stopword.<br>\n",
    "Line 10: Running a loop in the length of the sentence and then for each word in the sentence checking it in stopword and if it does not find in stopword then apply Stemming on to the text and add it to the list.<br>\n",
    "Line 11: Just concatenating all the words to make a sentence.<br>\n",
    "Line 12: appending the sentence to the corpus list.<br>\n",
    "Line 13: Printing the corpus list.<br>\n",
    "In Cleaning Process the next step is to convert the list of the sentence(corpus) into vectors so that we can feed this data into our machine learning model. for converting the text into vectors we are going to use a bag of words which is going to convert the text into binary form.’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "X = df['message']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3733,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=35000)\n",
    "X_train_vec = cv.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vec = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3733, 6177)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dumping the CV for future use**\n",
    "For doing predictions of new emails we need to save both our model and count vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle ## importing pickle used for dumping models\n",
    "pickle.dump(cv, open('cv.pkl', 'wb')) ## saving to into cv.pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(X_test_vec)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to evaluate our model using the confusion matrix and accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1585    8]\n",
      " [  15  231]] 98.74932028276237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "print(cm,score*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving our model\n",
    "\n",
    "To save our model we are going to use pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open(\"spam.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9874932028276238"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(\"spam.pkl\", \"rb\"))\n",
    "loaded_model.predict(X_test_vec)\n",
    "loaded_model.score(X_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction for a new email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter new review...it's not a good idea of implementing\n",
      "NOT SPAM\n"
     ]
    }
   ],
   "source": [
    "def new_review(new_review):\n",
    "    new_review = new_review\n",
    "    new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
    "    new_review = new_review.lower()\n",
    "    new_review = new_review.split()\n",
    "    ps = PorterStemmer()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    new_review = [ps.stem(word) for word in new_review if not word in   set(all_stopwords)]\n",
    "    new_review = ' '.join(new_review)\n",
    "    new_corpus = [new_review]\n",
    "    new_X_test = cv.transform(new_corpus).toarray()\n",
    "    new_y_pred = loaded_model.predict(new_X_test)\n",
    "    return new_y_pred\n",
    "new_review = new_review(str(input(\"Enter new review...\")))\n",
    "if new_review[0]==1:\n",
    "    print(\"SPAM\")\n",
    "else :\n",
    "    print(\"NOT SPAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter new review...\n",
    "IMPORTANT - You could be entitled up to £3,160 in compensation from mis-sold PPI on a credit card or loan. Please reply PPI for info or STOP to opt out. \n",
    "SPAM\n",
    "Enter new review...\n",
    "hi scott, i was wondering you have submitted your project or not\n",
    "NOT SPAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
